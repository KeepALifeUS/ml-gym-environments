# ML Gym Environments for Crypto Trading

Enterprise-grade OpenAI Gym environments Ğ´Ğ»Ñ cryptocurrency trading Ñ Context7 patterns Ğ¸ comprehensive sentiment analysis integration.

## ğŸ¯ Overview

This package implements sophisticated trading environments Ğ´Ğ»Ñ reinforcement learning agents, featuring:

- **Multi-Asset Trading**: Support Ğ´Ğ»Ñ multiple cryptocurrency pairs
- **Sentiment Analysis**: Integration of market sentiment from multiple sources
- **Market Microstructure**: Realistic order book simulation Ğ¸ market impact
- **Advanced Risk Management**: Comprehensive risk metrics Ğ¸ position sizing
- **Enterprise Patterns**: Production-ready logging, monitoring, Ğ¸ error handling
- **Context7 Compatibility**: Modern cloud-native architectural patterns

## ğŸš€ ML-Framework-1345 Implementation

This package directly addresses Jira task **ML-Framework-1345 - Implement Sentiment-Based Trading Signals** by providing:

1. **Sentiment-Enhanced Observations**: Multi-source sentiment data integration
2. **Regime-Aware Environments**: Market regime detection Ğ¸ adaptation
3. **Advanced Reward Functions**: Sentiment-aligned reward mechanisms
4. **Production-Ready Architecture**: Enterprise-grade implementation

## ğŸ“¦ Installation

```bash
# From ML-Framework project root
cd packages/ml-gym-environments
pip install -e .

# Or install dependencies directly
pip install gymnasium numpy pandas scipy

```

## ğŸ—ï¸ Architecture

```

ml-gym-environments/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environments/          # Core trading environments
â”‚   â”‚   â”œâ”€â”€ base_trading_env.py       # Abstract base class
â”‚   â”‚   â””â”€â”€ crypto_trading_env.py     # Crypto-specific environment
â”‚   â”œâ”€â”€ spaces/                # Observation & action spaces
â”‚   â”‚   â”œâ”€â”€ observations.py           # Multi-modal observations
â”‚   â”‚   â””â”€â”€ actions.py                # Trading action definitions
â”‚   â”œâ”€â”€ rewards/               # Reward function strategies
â”‚   â”‚   â”œâ”€â”€ profit_reward.py          # Profit-based rewards
â”‚   â”‚   â””â”€â”€ sharpe_reward.py          # Risk-adjusted rewards
â”‚   â”œâ”€â”€ simulation/            # Market simulation engine
â”‚   â”‚   â”œâ”€â”€ market_simulator.py       # Advanced market dynamics
â”‚   â”‚   â””â”€â”€ order_book.py             # Order book simulation
â”‚   â”œâ”€â”€ data/                  # Data management
â”‚   â”‚   â”œâ”€â”€ data_stream.py            # Real-time data streaming
â”‚   â”‚   â””â”€â”€ data_preprocessor.py      # Feature engineering
â”‚   â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”‚   â”œâ”€â”€ logger.py                 # Structured logging
â”‚   â”‚   â”œâ”€â”€ risk_metrics.py           # Risk calculations
â”‚   â”‚   â””â”€â”€ indicators.py             # Technical indicators
â”‚   â””â”€â”€ wrappers/              # Environment wrappers
â””â”€â”€ tests/                     # Comprehensive test suite

```

## ğŸ® Quick Start

### Basic Environment

```python
from ml_gym_environments import create_crypto_env, CryptoTradingConfig

# Create basic crypto environment
config = CryptoTradingConfig(
    assets=["BTC", "ETH", "BNB"],
    initial_balance=10000.0,
    max_steps=1000
)

env = create_crypto_env(config)

# Standard Gym interface
observation, info = env.reset()
for step in range(100):
    action = env.action_space.sample()  # Random action
    obs, reward, terminated, truncated, info = env.step(action)

    if terminated or truncated:
        break

env.close()

```

### Sentiment-Enhanced Environment

```python
from ml_gym_environments import create_sentiment_crypto_env

# Create environment Ñ sentiment analysis
env = create_sentiment_crypto_env(
    assets=["BTC", "ETH"],
    sentiment_sources=["twitter", "reddit", "news", "fear_greed_index"],
    sentiment_weight=0.2
)

# Environment includes sentiment Ğ² observations
observation, info = env.reset()
print(f"Observation shape: {observation.shape}")
print(f"Market info: {env.get_market_info()}")

```

### Advanced Configuration

```python
from ml_gym_environments import (
    CryptoTradingEnvironment,
    CryptoTradingConfig,
    ObservationConfig,
    ActionConfig,
    ActionMode
)

# Comprehensive configuration
obs_config = ObservationConfig(
    include_sentiment=True,
    include_order_book=True,
    include_technical_indicators=True,
    technical_indicators=["sma_20", "rsi_14", "macd", "bb_upper"]
)

action_config = ActionConfig(
    action_mode=ActionMode.CONTINUOUS,
    max_position_size=0.3,
    enable_limit_orders=True,
    enable_position_sizing=True,
    position_sizing_method="kelly"
)

trading_config = CryptoTradingConfig(
    assets=["BTC", "ETH", "BNB", "ADA"],
    enable_sentiment_signals=True,
    enable_order_book=True,
    enable_futures_trading=True,
    data_source="binance",
    observation_config=obs_config,
    action_config=action_config
)

env = CryptoTradingEnvironment(trading_config)

```

## ğŸ¯ Key Features

### 1. Multi-Modal Observations

The environments provide rich, multi-modal observations including:

- **Price Data**: OHLCV data Ñ configurable history length
- **Technical Indicators**: 20+ technical indicators
- **Sentiment Data**: Multi-source sentiment analysis
- **Market Microstructure**: Order book depth, bid-ask spreads
- **Portfolio State**: Current positions, balance, metrics
- **Market Regime**: Detected market conditions

### 2. Sophisticated Action Spaces

Multiple action space modes:

- **Discrete**: Simple buy/sell/hold actions
- **Continuous**: Position sizing Ñ risk constraints
- **Portfolio**: Target portfolio allocation
- **Orders**: Advanced order management (limit, stop, etc.)

### 3. Advanced Reward Functions

Enterprise-grade reward strategies:

```python
from ml_gym_environments import (
    create_simple_profit_reward,
    create_risk_adjusted_profit_reward,
    create_sharpe_reward,
    create_sortino_reward
)

# Profit-based reward
profit_reward = create_simple_profit_reward(
    profit_scale=100.0,
    enable_risk_penalty=True
)

# Risk-adjusted reward
sharpe_reward = create_sharpe_reward(
    lookback_window=50,
    target_sharpe=1.5
)

```

### 4. Market Simulation

Realistic market simulation Ñ:

- **Market Impact**: Sophisticated impact models
- **Slippage**: Volatility Ğ¸ size-based slippage
- **Liquidity**: Dynamic liquidity modeling
- **Latency**: Execution latency simulation
- **Partial Fills**: Realistic order execution

### 5. Enterprise Logging

Structured logging Ğ´Ğ»Ñ production:

```python
from ml_gym_environments import create_environment_logger

logger = create_environment_logger("crypto_env_001")

# Automatic trade logging
logger.log_trade_execution(
    asset="BTC",
    side="buy",
    quantity=0.1,
    price=45000.0,
    fees=45.0
)

# Performance metrics
logger.log_performance_metric("sharpe_ratio", 1.25)

# Export Ğ´Ğ»Ñ analysis
logger.export_events("trading_events.json")

```

## ğŸ“Š Observation Space

The observation space includes multiple feature categories:

| Category           | Features              | Description                  |
| ------------------ | --------------------- | ---------------------------- |
| **Prices**         | OHLCV history         | Historical price data        |
| **Technical**      | SMA, EMA, RSI, etc.   | Technical indicators         |
| **Sentiment**      | Twitter, Reddit, News | Sentiment scores             |
| **Microstructure** | Order book, trades    | Market microstructure        |
| **Portfolio**      | Positions, balance    | Current portfolio state      |
| **Regime**         | Bull/Bear/Volatile    | Market regime classification |

## âš¡ Performance Features

- **Async Support**: Real-time trading compatibility
- **Vectorized Environments**: Parallel execution
- **Memory Efficient**: Optimized data structures
- **Configurable**: Extensive configuration options
- **Monitoring**: Built-in performance monitoring

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test categories
pytest tests/test_environments.py -v
pytest tests/test_rewards.py -v
pytest tests/test_simulation.py -v

# Run Ñ coverage
pytest --cov=ml_gym_environments tests/

```

## ğŸ“ˆ Example Training Loop

```python
import gymnasium as gym
from ml_gym_environments import create_sentiment_crypto_env
from stable_baselines3 import PPO

# Create environment
env = create_sentiment_crypto_env(
    assets=["BTC", "ETH"],
    sentiment_sources=["twitter", "reddit", "news"]
)

# Train agent
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100000)

# Evaluate
obs, _ = env.reset()
for _ in range(1000):
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)

    if terminated or truncated:
        print(f"Episode finished. Portfolio value: ${info['portfolio_value']:,.2f}")
        obs, _ = env.reset()

```

## ğŸ”§ Configuration Options

### Environment Configuration

- **Assets**: List of crypto assets to trade
- **Data Source**: Historical, synthetic, Ğ¸Ğ»Ğ¸ live data
- **Market Features**: Order books, sentiment, technical indicators
- **Risk Management**: Position limits, stop losses, drawdown limits

### Observation Configuration

- **History Length**: Number of historical periods
- **Normalization**: Data normalization methods
- **Feature Selection**: Which features to include
- **Update Frequency**: How often to update features

### Action Configuration

- **Action Mode**: Discrete, continuous, Ğ¸Ğ»Ğ¸ portfolio
- **Position Sizing**: Fixed, Kelly, volatility-based
- **Order Types**: Market, limit, stop orders
- **Risk Constraints**: Maximum position sizes, leverage

## ğŸš€ Advanced Usage

### Custom Reward Functions

```python
from ml_gym_environments import BaseTradingEnvironment

class CustomReward:
    def calculate_reward(self, portfolio_value, previous_value, trade_info):
        # Custom reward logic
        profit = portfolio_value - previous_value
        volume_penalty = trade_info.get("total_fees", 0) * 0.5
        return profit - volume_penalty

# Use Ñ environment
reward_fn = CustomReward()
# Integrate Ğ² training loop...

```

### Multi-Environment Training

```python
from ml_gym_environments import create_crypto_env
from gymnasium.vector import AsyncVectorEnv

# Create multiple environments
def make_env():
    return create_crypto_env(CryptoTradingConfig(assets=["BTC", "ETH"]))

envs = AsyncVectorEnv([make_env for _ in range(4)])

# Train Ñ vectorized environments
# ... training code ...

```

## ğŸ“š Documentation

- **[API Reference](docs/api_reference.md)**: Complete API documentation
- **[Examples](examples/)**: Usage examples Ğ¸ tutorials
- **[Configuration Guide](docs/configuration.md)**: Detailed configuration options
- **[Architecture](docs/architecture.md)**: System architecture overview

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Make changes Ğ¸ add tests
4. Run tests: `pytest tests/`
5. Commit: `git commit -m 'Add amazing feature'`
6. Push: `git push origin feature/amazing-feature`
7. Create Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ·ï¸ Version

Current version: **1.0.0**

## ğŸ”— Related Packages

- **[ml-ppo](../ml-ppo/)**: PPO implementation Ğ´Ğ»Ñ crypto trading
- **[trading-engine](../trading-engine/)**: Core trading engine
- **[risk-manager](../risk-manager/)**: Risk management system
- **[common](../common/)**: Shared utilities Ğ¸ patterns

## ğŸ“ Support

For support Ğ¸ questions:

- Create an issue Ğ² the repository
- Check the [documentation](docs/)
- Review existing [examples](examples/)

---

**Built Ñ â¤ï¸ Ğ´Ğ»Ñ Crypto Trading Bot v5.0**

_Enterprise-grade trading environments powered by Context7 patterns_
